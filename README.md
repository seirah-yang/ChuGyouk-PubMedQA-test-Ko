# Llama-3.1-8B Fine-Tuning with Unsloth + TRL
	PubMedQA-Ko ë°ì´í„°ì…‹ ê¸°ë°˜ LoRA Instruction Fine-Tuning í”„ë¡œì íŠ¸

## 1. í”„ë¡œì íŠ¸ ê°œìš” (Project Overview)

ì´ í”„ë¡œì íŠ¸ëŠ” Meta-Llama-3.1-8B-Instruct (4bit, via Unsloth) ëª¨ë¸ì„ í•œêµ­ì–´ ì˜í•™ QA ë°ì´í„°ì…‹ì¸ PubMedQA-test-Koë¡œ íŒŒì¸íŠœë‹(fine-tuning)í•˜ì—¬ ì˜ë£Œ ê´€ë ¨ QA í’ˆì§ˆ í–¥ìƒê³¼ ë„ë©”ì¸ ì ì‘ì„ ì‹¤í—˜ ë° ê²€ì¦í•œ ëª¨ë¸ë§ì…ë‹ˆë‹¤.


## 2. ë°ì´í„°ì…‹ (Dataset)

	  1) Name: ChuGyouk/PubMedQA-test-Ko
	
	  2) Type: í•œêµ­ì–´ ë²ˆì—­ ë²„ì „ PubMedQA
	
	  3) Structure:
	 
		  (1) QUESTION :	ì˜í•™ì  ì§ˆë¬¸
		  
		  (2) CONTEXTS : ë…¼ë¬¸ ìš”ì•½(Abstract)
		  
		  (3) LONG_ANSWER : ë…¼ë¬¸ ë‚´ìš© ê¸°ë°˜ ì„œìˆ í˜• ë‹µë³€
	
	  4) test datasetì„ ì €ì¥ í›„, train/validation ë¶„í•  

## 3. ëª¨ë¸ êµ¬ì„±

	  1) Base model : unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit
	
	  2) Framework : PyTorch + TRL (SFTTrainer) + Unsloth
	
	  3) Task : Text Generation (ì§ˆë¬¸ + ë¬¸ë§¥ â†’ ë‹µë³€ ìƒì„±)
	
	  4) LoRA ì„¤ì • : r=16, Î±=16, dropout=0
	
	  5) Fine-tuning Layer : Q/K/V/Proj ê³„ì—´ Layer
``` python
# meta tensor materialize
  model = FastLanguageModel.for_training(model)
	
# LoRA ì„¤ì •
  model = FastLanguageModel.get_peft_model(
	    model,
	    r=16,
	    target_modules = ["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"],
	    lora_alpha=16,
	    lora_dropout=0,
	    bias="none",
	    use_gradient_checkpointing="unsloth",
	    random_state=3407,)
	    model = model.to("cuda")
```
	  6) Architecture :
	
		  (1) Input: QUESTION + CONTEXTS
		
		  (2) Output: LONG_ANSWER (ìì—°ì–´ ë¬¸ì¥)


## 4. í•™ìŠµ ì„¤ì • (Training Configuration)

	  1) Epochs : 2
	 
	  2) Learning rate : 2e-4
	 
	  3) Optimizer : AdamW 
	 
	  4) Scheduler : Linear Warmup
	 
	  5) Batch size : 3 (gradient_accum 4 â†’ total 12)
	 
	  6) Max steps : 60
	 
	  7) Precision : bf16 (ì§€ì› ì‹œ)
	 
	  8) Stability : Triton, CUDA Graph ë¹„í™œì„±í™”

## 5. WorkFlow 

	  1) ë°ì´í„°/ëª¨ë¸ ë¡œë“œ ë° LoRA êµ¬ì„±
	
```python
  from datasets import load_dataset
  ds = load_dataset("ChuGyouk/PubMedQA-test-Ko")
  df = ds["test"].to_pandas()
  df["input"] = "ì§ˆë¬¸: " + df["QUESTION"]
  df["output"] = df["LONG_ANSWER"]

  from unsloth import FastLanguageModel
  model, tokenizer = FastLanguageModel.from_pretrained(
  "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit",
	      max_seq_length=512,
	      dtype=None,
	      load_in_4bit=True,
	      device_map=None)
	
   model = FastLanguageModel.for_training(model)
```
	
	  2) ë°ì´í„° ì „ì²˜ë¦¬ (Alpaca í¬ë§· ë³€í™˜)
	 
	  3) í•™ìŠµ ì„¤ì • ë° Fine-Tuning
		    
```python 
   from trl import SFTTrainer
   from transformers import TrainingArguments
		   
   training_args = TrainingArguments(
		       output_dir="outputs",
		       per_device_train_batch_size=3,
		       gradient_accumulation_steps=4,
		       warmup_steps=5,
		       max_steps=60,
		       learning_rate=2e-4,
		       fp16=True,
		       eval_strategy="steps",
		       eval_steps=10,
		       report_to="none",
		       remove_unused_columns=False)
		   
   trainer = SFTTrainer(
	       model=model,
	       tokenizer=tokenizer,
	       train_dataset=train_ds,
	       eval_dataset=valid_ds,
	       dataset_text_field="text",
	       max_seq_length=512,
	       dataset_num_proc=2,
	       packing=False,
	       args=training_args)
		   
   trainer_stats = trainer.train()
```
	
	  4) ëª¨ë¸ ì €ì¥
		 
```python 
   FastLanguageModel.for_inference(model)
   model.save_pretrained("lora_model_llama3")
   tokenizer.save_pretrained("lora_model_llama3")
   model.save_pretrained_merged("lora_model_llama3_merged", tokenizer, save_method="merged_16bit")
```
	
	  5) ê²€ì¦ ë° í…ŒìŠ¤íŠ¸
	 
```python 
   from unsloth import FastLanguageModel
   import torch
   
   model, tokenizer = FastLanguageModel.from_pretrained(
			  "lora_model_llama3_merged",
    		  max_seq_length=512,
		      dtype=torch.float16,
		      load_in_4bit=False,
		      device_map=None)
		   
   prompt = "ë°°ë³€ ì‹œ í•¸ë“œí° ì‚¬ìš© ìŠµê´€ì´ ì§ì¥ì•”ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€?"
   inputs = tokenizer(prompt, return_tensors="pt").to("cuda:0")
   with torch.no_grad():
   outputs = model.generate(**inputs, max_new_tokens=128)
   print("ğŸ’¬ ëª¨ë¸ ì‘ë‹µ:", tokenizer.decode(outputs[0], skip_special_tokens=True))
``` 
   
## 6. ê²°ê³¼
	  1) ì •ëŸ‰ì /ì •ì„±ì  ì„±ëŠ¥ í‰ê°€
		  (1) Validation loss 1.24 â†’ Test loss 1.21ë¡œ ì•ˆì • ìˆ˜ë ´
		
		  (2) ì •ì„±ì  í’ˆì§ˆ : Instruction ê¸°ë°˜ ì‘ë‹µì˜ ì¼ê´€ì„± í–¥ìƒ, ì˜ë£Œìš©ì–´ í‘œí˜„ ìì—°ìŠ¤ëŸ¬ì›€
		  
		  (3) ê²°ê³¼ íŠ¹ì§• : PubMedQA-Ko ë°ì´í„°ì—ì„œ ì„ìƒ ì§ˆë¬¸ ì´í•´ë ¥ í–¥ìƒ(ì˜í•™ì  ì •í™•ë„ì™€ ë¬¸ì²´ ì¼ê´€ì„± í–¥ìƒ)
		
		  (4) Error ì‚¬ë¡€ : ê¸´ ë¬¸ì¥ì—ì„œ ë¬¸ë‹¨ ì „í™˜ì´ ë¶€ìì—°ìŠ¤ëŸ¬ì›€ â†’ max_length=512 ì œí•œ ì˜í–¥(ìš©ëŸ‰ë¬¸ì œë¡œ ì œí•œ í•¨)
	
	  
	  2) í…ŒìŠ¤íŠ¸ ê²°ê³¼
          (1) ì˜í•™ì  ë¬¸ì²´ ë° ë…¼ë¬¸ì‹ ì„œìˆ  ë¬¸ì²´ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µ ìƒì„±
![img:Resul Answer](URL:https://github.com/seirah-yang/LLM_finetuning/blob/main/LLM_answer.png)

## 7. Summary & Research Plan

ë³¸ ì—°êµ¬ëŠ” í•œêµ­ì–´ ì˜í•™ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ(QA) ë°ì´í„°ì…‹ì¸ PubMedQA-Koë¥¼ í™œìš©í•˜ì—¬ Meta-Llama-3.1-8B-Instruct (LoRA 4bit) ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•¨ìœ¼ë¡œì¨, â€œì˜ë£Œ ë°ì´í„° + AI + í’ˆì§ˆ ê´€ë¦¬â€ì˜ êµì°¨ì ì—ì„œ ì„ìƒì—°êµ¬ ë°ì´í„° ë¶„ì„ ë° ì‘ë‹µ ë°œí˜„ ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ì˜€ìŠµë‹ˆë‹¤.
	
ì´ë¥¼ í†µí•´ ëª¨ë¸ì€ ì˜í•™ì  ì§ˆë¬¸ì˜ ì´í•´ë ¥, ë¬¸ì²´ì˜ ì¼ê´€ì„±, ì˜ë£Œìš©ì–´ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ì‚¬ìš© ëŠ¥ë ¥ì„ í–¥ìƒì‹œì¼°ìœ¼ë©°, í–¥í›„ ë¶„ì‚°í˜• ì„ìƒì—°êµ¬(DCT), eCRF ë°ì´í„° í’ˆì§ˆê²€ì¦, ì„ìƒë¬¸ì„œ ìƒì„± ë° ìë™ê²€ì¦ AI ì‹œìŠ¤í…œìœ¼ë¡œì˜ í™•ì¥í•˜ëŠ” ê²ƒì„ ì œì–¸í•©ë‹ˆë‹¤. 

	1) DCT ë¶„ì‚°í˜• ì„ìƒ ì—°êµ¬ë¥¼ ìœ„í•œ ì¦ìƒ ëª¨ë‹ˆí„°ë§: í™˜ì ìê°€ ë³´ê³ ìš© triage ë¶„ë¥˜ 
	  	(1) ë‹¨ê³„ 
	    
		    â‘  ì¦ìƒ ëª¨ë‹ˆí„°ë§ ìë™ triage
		    
		      : í™˜ì ìê°€ ë³´ê³  ë°ì´í„° ê¸°ë°˜ ì´ìƒ ì¦ìƒ ìë™ ë¶„ë¥˜
		    
		    â‘¡ AE ë…¸íŠ¸ - triage + ì½”ë”© Proof of Concept
		    
		      : CDASH/SDTM í‘œì¤€ ê¸°ì¤€ì— ê¸°ë°˜í•˜ì—¬ ì˜í•™ ìš©ì–´ ë§¤í•‘ ë° í’ˆì§ˆ ê²€ì¦ 
		    
		    â‘¢ ê·œì • RAG + ê²€ì¦ + ë¬¸ì„œìƒì„± 
		    
		      : ê·œì •ì— ì¤€ìˆ˜í•˜ì—¬ í™˜ì ì¦ìƒ ëª¨ë‹ˆí„°ë§ ë° ë³´ê³ , EHR ì‘ì„± í›„, ìë™ ê²€ì¦ ì‹œìŠ¤í…œ êµ¬ì¶•

  	2) í™•ì¥ ë° í™œìš© ë°©ì•ˆ
		(1) ì˜ë£Œ QA ìë™í™” 
	     	- ì„ìƒì—°êµ¬ ê°„í˜¸ì‚¬(CRC/CRA) ë° ì—°êµ¬ì ëŒ€ìƒ ì˜ë£Œ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ êµ¬ì¶•
	     	- PubMedQA Fine-tuning ê²°ê³¼ ê¸°ë°˜ ì„ìƒì‹œí—˜ í”„ë¡œí† ì½œ, SAE ë³´ê³ ì„œ ì‘ì„± ë° ì§ˆì˜ì‘ë‹µ ìë™í™”
	
		(2) DCT ê¸°ë°˜ ëª¨ë‹ˆí„°ë§
	   		- í™˜ì ìê°€ ë³´ê³ í˜• ì¦ìƒ ë°ì´í„° triage ì‹œìŠ¤í…œ êµ¬ì¶•
	      	- í™˜ìê°€ ëª¨ë°”ì¼ ë˜ëŠ” ì›¨ì–´ëŸ¬ë¸”ì„ í†µí•´ ë³´ê³ í•œ ì¦ìƒì„ ìë™ ë¶„ë¥˜
			- ì´ìƒ ì¦ìƒ(AE) ë°œìƒ ì‹œ ì¦‰ì‹œ ì•Œë¦¼ ë° ì˜ë£Œì§„ ê²€í†  ì§€ì›
	   	
		(3) eCRF ì—°ê³„
	   		- CDASH/SDTM í‘œì¤€ í¬ë§· ë§¤í•‘ ë° AI ê¸°ë°˜ ì§ˆì˜ ìƒì„±
	    	- í™˜ì ë³´ê³  ë°ì´í„°ë‚˜ ëª¨ë‹ˆí„°ë§ ë¡œê·¸ eCRF êµ¬ì¡°ë¡œ ë³€í™˜
			- EHRì˜ ì´ìƒì¹˜ ë˜ëŠ” ê²°ì¸¡ì¹˜ ìë™ íƒì§€ ë° ì§ˆì˜ ìë™ ìƒì„±
	    
		(4) ë¬¸ì„œ í’ˆì§ˆê´€ë¦¬
	        - í–‰ì •/R&D ë¬¸ì„œ ìë™ ê²€ì¦ ë° ì°¸ê³ ë¬¸í—Œ ë§í¬ ì œì‹œ ì‹œìŠ¤í…œ
	        - ê·œì • ê¸°ë°˜ RAGê³¼ NLI ê¸°ë°˜ ê²€ì¦ê¸°ë¥¼ ì—°ê³„
			- ë¬¸ì„œì˜ í¬ë§·Â·ê·œì •Â·ì •í•©ì„±ì„ í‰ê°€ ë° ì°¸ì¡° ë¬¸í—Œ ìë™ ë§í¬
	
	    (5) ë„ë©”ì¸ í™•ì¥
			- Oncology, Pharmacovigilance, CDISC í‘œì¤€ ê¸°ë°˜ í•™ìŠµ í™•ì¥
	      	- ì•”í™˜ì ì¦ìƒ ë°ì´í„°, ì•½ë¬¼ ë¶€ì‘ìš© ëª¨ë‹ˆí„°ë§
			- ì˜í•™ í†µê³„ ë¶„ì„ ë“± ë‹¤ì–‘í•œ ì„ìƒ ë°ì´í„° ë„ë©”ì¸ í™•ì¥ ê°€ëŠ¥
        
### Author  
	** ì–‘ ì†Œ ë¼ | RN, BSN, MSN **
	
	Clinical Data Science Researcher
	
	AI Developer (End-to-End Clinical AI Bootcamp, AlphaCo)
	
	Domain Focus: Clinical Data Management & Digital Medicine
					- DCT
					- CDISC/CDASH
					- AI for eCRF & NLP-based Document Automation

### ì°¸ê³ ë¬¸í—Œ 
	 â€¢ GPT-2ë¥¼ ì´ìš©í•œ ì±—ë´‡ íŒŒì¸íŠœë‹. https://wikidocs.net/217620
	 
	 â€¢ Hugging Face TRL Docs
		
	 â€¢ Jin et al. (2019). PubMedQA: A Dataset for Biomedical Research Question Answering. EMNLP 2019.
		
	 â€¢ LGAI Research (2024). EXAONE 3.5 Multilingual Large Language Model.
	 
	 â€¢ Unsloth Docs â€” Efficient LoRA Training
	 
	 â€¢ ë”¥ëŸ¬ë‹ ì „ì´í•™ìŠµ(Transfer Learning)ê³¼ íŒŒì¸íŠœë‹(Fine Tuning) â†’ https://hi-ai0913.tistory.com/32
