# Llama-3.1-8B Fine-Tuning with Unsloth + TRL
PubMedQA-Ko ë°ì´í„°ì…‹ ê¸°ë°˜ LoRA Instruction Fine-Tuning í”„ë¡œì íŠ¸

## 1. í”„ë¡œì íŠ¸ ê°œìš” (Project Overview)

ì´ í”„ë¡œì íŠ¸ëŠ” Meta-Llama-3.1-8B-Instruct ëª¨ë¸ì„ Unsloth(4bit LoRA) ë° Hugging Face TRL(SFTTrainer) ê¸°ë°˜ìœ¼ë¡œ íŒŒì¸íŠœë‹í•˜ì—¬ í•œê¸€ ì˜ë£Œ ì§ˆì˜ì‘ë‹µ ë°ì´í„°ì…‹(PubMedQA-test-Ko)ì— íŠ¹í™”ëœ ì„œìˆ  ë‹µë³€(Long Answer)ì„ ìƒì„±í•˜ë„ë¡ í•™ìŠµ(fine-tuning)í•˜ëŠ” í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

## 2. ë°ì´í„°ì…‹ (Dataset)

 1) Name: ChuGyouk/PubMedQA-test-Ko

 2) Type: í•œêµ­ì–´ ë²ˆì—­ ë²„ì „ì˜ PubMedQA

 3) Structure:
 
  (1) QUESTION :	ì˜í•™ì  ì§ˆë¬¸
  
  (2) CONTEXTS : ê´€ë ¨ ë…¼ë¬¸ ìš”ì•½(Abstract)
  
  (3) LONG_ANSWER : ë…¼ë¬¸ ë‚´ìš© ê¸°ë°˜ ì„œìˆ í˜• ë‹µë³€

## 3. ì£¼ìš”êµ¬ì„±

 1) Base model: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit

 2) Framework: PyTorch + Hugging Face Transformers

 3) Task: Text Generation (ì§ˆë¬¸ + ë¬¸ë§¥ â†’ ë‹µë³€ ìƒì„±)

 4) Architecture:

  (1) Input: QUESTION + CONTEXTS

  (2) Output: LONG_ANSWER (ìì—°ì–´ ë¬¸ì¥)

  (3) Fine-tuning scope: ìƒìœ„ 7ê°œ Transformer layer + LM head

 5) í•™ìŠµ ë°©ë²• : LoRA (Low-Rank Adaptation) + Supervised Fine-Tuning (SFT)
 
``` python
   # meta tensor materialize
   model = FastLanguageModel.for_training(model)

   # LoRA ì„¤ì •
   model = FastLanguageModel.get_peft_model(
       model,
       r=16,
       target_modules = ["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"],
       lora_alpha=16,
       lora_dropout=0,
       bias="none",
       use_gradient_checkpointing="unsloth",
       random_state=3407,)
   model = model.to("cuda")
```

 6) í™˜ê²½ ì„¤ì • 
  ``` python
   !pip install -U "unsloth>=0.8.8" "trl>=0.9.4" "transformers>=4.44.0" "accelerate>=0.33.0" "bitsandbytes>=0.43.1" "datasets" "scikit-learn"
   !pip uninstall -y peft && pip install "peft>=0.11.1"
```
â˜ í•˜ìœ„ì¸µì€ ì–¸ì–´ ì¼ë°˜ ì§€ì‹ ìœ ì§€, ìƒìœ„ì¸µë§Œ ì˜í•™ ë„ë©”ì¸ì— ë§ê²Œ ë¯¸ì„¸ì¡°ì •

## 4. í•™ìŠµ ì„¤ì •(Training Configuration)
 1) Epochs : 3
 
 2) Learning rate :	1e-5
 
 3) Optimizer :	AdamW
 
 4) Batch size : 1
 
 5) Precision : bfloat16
 
 6) Framework	PyTorch 2.x / Transformers 4.40+

## 5. WorkFlow 

 1) ëª¨ë¸ ë¡œë“œ ë° LoRA êµ¬ì„±
 
 2) ë°ì´í„° ì „ì²˜ë¦¬ (Alpaca í¬ë§· ë³€í™˜)
 
 3) í•™ìŠµ ì„¤ì • ë° Fine-Tuning
    
  ```python 
   from trl import SFTTrainer
   from transformers import TrainingArguments
   
   training_args = TrainingArguments(
       output_dir="outputs",
       per_device_train_batch_size=3,
       gradient_accumulation_steps=4,
       warmup_steps=5,
       max_steps=60,
       learning_rate=2e-4,
       fp16=True,
       eval_strategy="steps",
       eval_steps=10,
       report_to="none",
       remove_unused_columns=False)
   
   trainer = SFTTrainer(
       model=model,
       tokenizer=tokenizer,
       train_dataset=train_ds,
       eval_dataset=valid_ds,
       dataset_text_field="text",
       max_seq_length=512,
       dataset_num_proc=2,
       packing=False,
       args=training_args)
   
   trainer_stats = trainer.train()
 ```

 4) ëª¨ë¸ ì €ì¥
```python 
  # LoRA ì–´ëŒ‘í„° ì €ì¥
model.save_pretrained("lora_model_llama3")
tokenizer.save_pretrained("lora_model_llama3")
```

 5) í…ŒìŠ¤íŠ¸ í‰ê°€ ë° ì¶”ë¡ 
 ```python 
   # test_dsì—ì„œ text ì»¬ëŸ¼ë§Œ ì‚¬ìš©
   test_ds = test_ds.select_columns(["text"])
   trainer.args.remove_unused_columns = False
   
   eval_results = trainer.evaluate(test_ds)
   print("\nğŸ“Š Test Evaluation Results:")
   print(eval_results)
   
   # ìƒ˜í”Œ ì¶”ë¡ 
   prompt = "íì•” í™˜ìì˜ í†µì¦ ê´€ë¦¬ë¥¼ ìœ„í•´ ê°€ì¥ ì¤‘ìš”í•œ í‰ê°€ í•­ëª©ì€ ë¬´ì—‡ì¸ê°€ìš”?"
   inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
   outputs = model.generate(**inputs, max_new_tokens=200)
   print(tokenizer.decode(outputs[0], skip_special_tokens=True))
 ```
 6) ë¶„ì„ & ì‹œê°í™”
 ```python 
 # 1. ë°ì´í„° ë¡œë“œ
from datasets import load_dataset
ds = load_dataset("ChuGyouk/PubMedQA-test-Ko")

# 2. í•™ìŠµ ì‹¤í–‰
python train_pubmedqa_exaone.py 

## training 
optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)
num_epochs = 3
total_steps = len(train_loader) * num_epochs
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=int(0.1 * total_steps),
    num_training_steps=total_steps
)

model.train()
for epoch in range(num_epochs):
    total_loss = 0
    for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
        optimizer.zero_grad()
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1} - Loss: {total_loss/len(train_loader):.4f}")

```
## 6. Summary & Research Plan
ë³¸ ë°ì´í„°ëŠ” í•œêµ­ì–´ ì˜í•™ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ(QA) ë°ì´í„°ì…‹ì„ EXAONE-3.5-2.4Bë¥¼ PubMedQA-Koì— íŠ¹í™”ì‹œì¼œ â€œì˜ë£Œ ë°ì´í„° + AI + í’ˆì§ˆ ê´€ë¦¬â€ êµì°¨ì ì—ì„œ ì„ìƒì—°êµ¬ ë°ì´í„° ë¶„ì„ ë° ì‘ë‹µ ë°œí˜„ ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ì˜€ìŠµë‹ˆë‹¤.
í–¥í›„ DCT(ë¶„ì‚°í˜• ì„ìƒì—°êµ¬), eCRF, ì„ìƒë¬¸ì„œ ìƒì„± ë° ê²€ì¦ AI ì‹œìŠ¤í…œìœ¼ë¡œ ê°œë°œë¡œì˜ í™•ì¥ì„ ë‹¤ìŒê³¼ ê°™ì´ ì œì–¸í•©ë‹ˆë‹¤. 

 ### 1) DCT ë¶„ì‚°í˜• ì„ìƒ ì—°êµ¬ë¥¼ ìœ„í•œ ì¦ìƒ ëª¨ë‹ˆí„°ë§: í™˜ì ìê°€ ë³´ê³ ìš© triage ë¶„ë¥˜ 
   (1) ë‹¨ê³„ 
      - ì¦ìƒ ëª¨ë‹ˆí„°ë§ ìë™ triage
        : í™˜ì ìê°€ ë³´ê³  ë°ì´í„° ê¸°ë°˜ ì´ìƒ ì¦ìƒ ìë™ ë¶„ë¥˜
      - AE ë…¸íŠ¸ - triage + ì½”ë”© Proof of Concept
        : CDASH/SDTM í‘œì¤€ ê¸°ì¤€ì— ê¸°ë°˜í•˜ì—¬ ì˜í•™ ìš©ì–´ ë§¤í•‘ ë° í’ˆì§ˆ ê²€ì¦ 
      - ê·œì • RAG + ê²€ì¦ + ë¬¸ì„œìƒì„± 
        : ê·œì •ì— ì¤€ìˆ˜í•˜ì—¬ í™˜ì ì¦ìƒ ëª¨ë‹ˆí„°ë§ ë° ë³´ê³ , EHR ì‘ì„± í›„, ìë™ ê²€ì¦ ì‹œìŠ¤í…œ êµ¬ì¶• 
        
** Author ** 
ì–‘ ì†Œ ë¼ | RN, BSN, MSN

**ì°¸ê³  ë¬¸í—Œ**
ë”¥ëŸ¬ë‹ ì „ì´í•™ìŠµ(Transfer Learning)ê³¼ íŒŒì¸íŠœë‹(Fine Tuning) â†’ https://hi-ai0913.tistory.com/32
GPT-2ë¥¼ ì´ìš©í•œ ì±—ë´‡ íŒŒì¸íŠœë‹ â†’ https://wikidocs.net/217620
Hu et al., LoRA: Low-Rank Adaptation of Large Language Models (ICLR 2022)
Ouyang et al., InstructGPT: Training language models to follow instructions with human feedback (2022)
Unsloth Docs â€” Efficient LoRA Training
Hugging Face TRL Docs
